{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5 - Object Detection\n",
    "\n",
    "### Due Date: *Friday, April 5th*\n",
    "\n",
    "### What to Submit\n",
    "Submit this iPython Notebook--containing all your code for the programming exercises below--on [LearningSuite](https://learningsuite.byu.edu/). <span style=\"color:red\">PLEASE ONLY SUBMIT THE NOTEBOOK -- I DON'T NEED ANY OF THE OTHER SUPPORTING FILES OR DATA</span>\n",
    "\n",
    "Your notebook file should produce the relevant plots and also provide a short write-up with answers to the questions below (at the end of this notebook).\n",
    "\n",
    "Please also fill in here the time that each part took you:\n",
    "* 1. Part A - Feature Extraction: <span style=\"color:red;\">10 minutes (mostly waiting for it to load)</span>\n",
    "* 2. Part B - Vector Quantization: <span style=\"color:red;\">15 minutes (mostly waiting for it to load)</span>\n",
    "* 3. Part C - Classifier Training: <span style=\"color:red;\">2 minutes</span>\n",
    "* 4. Part D - Sliding Window Detection: <span style=\"color:red;\">n/a</span>\n",
    "* 5. Part E - Evaluation: <span style=\"color:red;\">1 hour 30 minutes</span>\n",
    "* 6. Challenges: <span style=\"color:red;\">n/a</span>\n",
    "* 7. Write-up: <span style=\"color:red;\">10 minutes</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background - Object Detection\n",
    "\n",
    "For this assignment, you will implement an object detection pipeline. Your project will leverage a set of training images (both positive examples where the object of interest is present and negative examples where it is not).  Your program will divide a query/test image up into image regions or _windows_ and convert each window into a feature vector representation (based on Bag-of-Words and/or HOG).  Using the training data, you will use a machine-learning algorithm to train a classification model which will predict the probability of each window (its derived feature vector, actually) containing the object to be detected.\n",
    "\n",
    "This project will apply several of the things you’ve done previously, but also incorporate the idea of supervised machine learning.  You will need to implement both the feature extraction and the classification parts of the pipeline.  Here are the steps that you will follow:\n",
    "\n",
    "1) Extract and cluster feature descriptors - You will extract features (feature _descriptors_) from a variety of images and cluster them into $K$ discrete clusters (visual words).\n",
    "\n",
    "2) Vector Quantization - Given the visual vocabulary, each image is converted into a Bag-of-Words representation (a $K$-dimensional histogram).\n",
    "\n",
    "3) Train Detection Classifier - Given the positive and negative training examples, a Support Vector Machine (SVM) is trained to distinguish between the object and everything else.\n",
    "\n",
    "4) ~Sliding Window - Finds the actual object in an image by iterative considering windows at different locations and scales.~ <span style=\"color:red\">We will not be doing sliding window detection.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this project, we will provide you with a dataset comprised of Birds and Flowers.  Your task will be to detect the birds that are present in a set of test images.\n",
    "\n",
    "1. Within the main folder, download and setup the CUB-200-2011 (1.1GB) dataset:\n",
    "```\n",
    "    wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz\n",
    "    tar xvfz CUB_200_2011.tgz\n",
    "    ln -s CUB_200_2011/images .    OR    mv CUB_200_2011/images .\n",
    "``` \n",
    "\n",
    "\n",
    "2. Do the same for the Oxford-Flowers-102 (350MB) dataset:\n",
    "```\n",
    "    wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
    "    tar xvfz 102flowers.tgz\n",
    "    [the jpg folder should be right there in the directory]\n",
    "```\n",
    "\n",
    "3. You should now be able to access the Datasets from within your notebook using the following syntax/commands:\n",
    "\n",
    "```Python\n",
    "import cub_dataset\n",
    "cub_data = cub_dataset.CUB_Dataset()\n",
    "cub_train = cub_dataset.getTrainingSet()\n",
    "cub_test = cub_dataset.getTestSet()\n",
    "\n",
    "for imgnum in cub_train:\n",
    "    kp,desc = cub_data.getSIFTfeatures(imgnum,bbox_only=True) # Get only the features within the bounding box\n",
    "\n",
    "# If you wanted to display images, you can get them with...\n",
    "img1 = cub_train.getImage(1,bbox_only=False) # Get the whole image\n",
    "img2 = cub_train.getImage(1,bbox_only=True)  # Get only the bounding box\n",
    "\n",
    "\n",
    "import flowers_dataset\n",
    "flower_data  = flowers_dataset.Flowers_Dataset()\n",
    "flower_train = flower_data.getTrainingSet()\n",
    "flower_test = flower_data.getTestSet()\n",
    "\n",
    "for imgnum in flower_train:\n",
    "    kp,desc = flower_data.getSIFTfeatures(imgnum) # The flowers dataset does not include bounding boxes.\n",
    "```\n",
    "\n",
    "**Test Set A is composed from these two datasets.** Specifically, it is composed of (i) the positive images, namely those in `cub_data.getTestSet()` and, (ii) the negative images, namely those in `flower_data.getTestSet()`.\n",
    "\n",
    "~Test Set B is provided with the lab files and contains images with multiple birds in them.~ <span style=\"color:red\">Test Set B is for sliding window detection, which we will not be doing.</span>\n",
    "\n",
    "**Note:** When you are first implementing the lab, you may find it helpful to only load a subset of the images (ex. the first 100). This will make your code run faster and easier to debug. You can then increase the number of images for your final results once you have implemented the full lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Extract and Cluster Feature Descriptors\n",
    "\n",
    "To create the visual vocabulary, you will need to sample (grab a subset) from amongst all the sift descriptors of a large number of images. You want a representative set of descriptors, so you should ideally grab from images of all categories you intend to recognize and also from both the set of\n",
    "training images and the set of test images. A useful routine for sampling is `numpy.random.permutation(n)[0:m]` which will generate a random subset of $m$ numbers between $0$ and $n-1$. Generate a vector of column indices in this way and use it to grab a subset of $m$ rows from a given image’s descriptor matrix.\n",
    "\n",
    "So, once you have $m$ descriptors from each image, you will concatenate these together into a single matrix. Each row is a single descriptor, so with SIFT features, this matrix has 128 columns. Let us suppose that there are a total of $M$ rows/descriptors.\n",
    "\n",
    "Now this matrix represents $M$ distinct points in $R^{128}$. The k-means algorithm will partition these $M$ 128-dimensional points into $K$ clusters, for some value $K$ that is provided. $K$ determines how many words will be in our visual vocabulary. So, you will take his large $M$ x 128 matrix, pass it to k-means along with the desired number of clusters $K$, and kmeans will return a set of $K$ 128-dimensional points each representing the center of one of the clusters. These $K$ points form your visual vocabulary.\n",
    "\n",
    "Resources that you may find useful:\n",
    "* [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) [[Examples]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#examples-using-sklearn-cluster-kmeans)\n",
    "* [sklearn.cluster.MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import cub_dataset, flowers_dataset\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from functools import partial\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class ElapsedTime(threading.Thread):\n",
    "    '''A context manager for timing long-running functions.\n",
    "    \n",
    "    Args:\n",
    "        message: a prefix message to print along with the elapsed time.\n",
    "    \n",
    "    Usage:\n",
    "        with ElapsedTime(\"This message will be printed\") as track:\n",
    "            someFunction()  # keep track of how long this function takes\n",
    "            track.stop()  # IMPORTANT: you must call stop()\n",
    "    '''\n",
    "    def __init__(self, message):\n",
    "        super(ElapsedTime, self).__init__()\n",
    "        self.message = message\n",
    "        self._stop_event = threading.Event()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_event.set()\n",
    "\n",
    "    def stopped(self):\n",
    "        return self._stop_event.is_set()\n",
    "\n",
    "    def run(self):\n",
    "        thread_start = time.time()\n",
    "        while not self.stopped():\n",
    "            print(f\"\\r{self.message} {time.time()-thread_start:.1f} seconds\", end=\"\")\n",
    "            time.sleep(.999)\n",
    "        print()\n",
    "        self.elapsed = time.time()-thread_start\n",
    "            \n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.stop()\n",
    "        self.join()\n",
    "        return type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is provided for you to sample image descriptors. Please look\n",
    "# through it and understand what it does.\n",
    "def sample_descriptors(dataset, img_ids, m=100):\n",
    "    '''Sample descriptors from images in dataset.\n",
    "    \n",
    "    args:\n",
    "        dataset: the dataset to sample from (cub_dataset.CUB_Dataset or\n",
    "            flowers_dataset.Flowers_Dataset)\n",
    "        img_ids: list of integer image ids - the set of images to sample from\n",
    "        m (int): the max number of descriptors to sample from each image\n",
    "        \n",
    "    returns:\n",
    "        descriptors: a numpy array containing up to n*m rows (n images and up\n",
    "            to m descriptors from each) and 128 columns.\n",
    "    '''\n",
    "    descriptors = []\n",
    "    for i in tqdm.tqdm(img_ids, desc=dataset.__class__.__name__):\n",
    "        kp, desc = dataset.getSIFTfeatures(i)\n",
    "        n_d = desc.shape[0]\n",
    "        ind = np.random.choice(n_d, min(m, n_d), replace=False)\n",
    "        descriptors.append(desc[ind])\n",
    "    descriptors = np.concatenate(descriptors, 0)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP THE DATA\n",
    "cub_data = cub_dataset.CUB_Dataset()\n",
    "cub_train = cub_data.getTrainingSet()\n",
    "cub_test = cub_data.getTestSet()\n",
    "cub_data.getSIFTfeatures = partial(cub_data.getSIFTfeatures, bbox_only=True)\n",
    "\n",
    "flower_data = flowers_dataset.Flowers_Dataset()\n",
    "flower_train = flower_data.getTrainingSet()\n",
    "flower_test = flower_data.getTestSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUB_Dataset: 100%|███████████████████████████| 100/100 [00:11<00:00,  8.89it/s]\n",
      "Flowers_Dataset: 100%|███████████████████████| 100/100 [00:22<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# YOU MAY WANT TO TRY THINGS OUT WITH A SUBSET OF THE IMAGES. IT TAKES A WHILE\n",
    "# TO RUN THINGS WITH THE FULL SET - BUT YOU'LL PROBABLY GET BETTER RESULTS BY\n",
    "# USING MORE IMAGES\n",
    "\n",
    "m = 100  # number of descriptors to sample from each image\n",
    "num_img = 100 # None  # number of images to use from each dataset - set to None to use all\n",
    "\n",
    "# UNCOMMENT THESE LINES IF YOU WANT TO GENERATE A NEW SET OF DESCRIPTORS\n",
    "cub_descriptors = sample_descriptors(cub_data, cub_train[:num_img], m) \n",
    "flower_descriptors = sample_descriptors(flower_data, flower_train[:num_img], m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THESE LINES IF YOU WANT TO SAVE THE DESCRIPTORS YOU GENERATED\n",
    "np.save('user-data/cub-descriptors.npy', cub_descriptors)\n",
    "np.save('user-data/flower-descriptors.npy', flower_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6186fccaef41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# LOAD SAVED DESCRIPTORS - THEY MUST BE GENERATED FIRST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcub_descriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user-data/cub-descriptors.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mflower_descriptors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user-data/flower-descriptors.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcub_descriptors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflower_descriptors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\lab3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 453\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    454\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\lab3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LOAD SAVED DESCRIPTORS - THEY MUST BE GENERATED FIRST\n",
    "cub_descriptors = np.load('user-data/cub-descriptors.npy')\n",
    "flower_descriptors = np.load('user-data/flower-descriptors.npy')\n",
    "print(cub_descriptors.shape)\n",
    "print(flower_descriptors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "                init_size=None, max_iter=100, max_no_improvement=10,\n",
       "                n_clusters=200, n_init=3, random_state=None,\n",
       "                reassignment_ratio=0.01, tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLUSTER DESCRIPTORS\n",
    "# Use MiniBatchKMeans to cluster all the training descriptors (combine the \n",
    "# cub and flower descriptors)\n",
    "# You can play with different values of k (number of clusters)\n",
    "# This may take some time to run. You can try increasing the batch size - it\n",
    "# may speed things up\n",
    "# You can use the provided ElapsedTime context manager to keep track of how\n",
    "# long it takes to run\n",
    "\n",
    "# clustering code goes here\n",
    "all_descriptors = np.concatenate([cub_descriptors, flower_descriptors], axis=0)\n",
    "kmeans = MiniBatchKMeans(200)\n",
    "kmeans.fit(all_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN SAVE THE STATE OF THE KMEANS OBJECT TO LOAD LATER\n",
    "# SO YOU DON'T HAVE TO RERUN IT\n",
    "pickle.dump(kmeans, open('user-data/kmeans-state.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU LOAD IT LIKE SO\n",
    "kmeans = pickle.load(open('user-data/kmeans-state.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Vector Quantization - Represent each Image as a Bag-of-Words\n",
    "\n",
    "Vector Quantization is the process of taking this visual vocabulary (the $K$ cluster centers) and converting the list of descriptors representation into a bag of visual words, a histogram identifying, for each visual word (each bin in the histogram), the fraction of the SIFT descriptors for which that visual word, or cluster center, is the closest one (in the 128-dimensional space).\n",
    "\n",
    "Each image is thus converted using the visual vocabulary into a histogram with $K$ bins, or equivalently a feature vector with $K$ dimensions. Each descriptor in an image is mapped as a vote for the cluster whose center that given descriptor is closest to.\n",
    "\n",
    "You can use the KMeans object you trained (fit) in Part A to predict which visual word/cluster each given descriptor is closest to.  Aggregating the cluster predictions for all descriptors in an image or window produces the Bag-of-Visual-Words representation ($K$-dimensional histogram) for the image/window.\n",
    "\n",
    "\n",
    "![alt text](pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is provided for you to compute bag-of-words descriptors. Please\n",
    "# look through it and understand what it does. Take a minute to look up\n",
    "# np.bincount and understand it.\n",
    "def get_bows(dataset, img_ids, kmeans):\n",
    "    '''Get a bag-of-words representation for a list of images in a dataset.\n",
    "    \n",
    "    args:\n",
    "        dataset: the dataset to sample from (cub_dataset.CUB_Dataset of\n",
    "            flowers_dataset.Flowers_Dataset)\n",
    "        img_ids: list of integer image ids - the set of images to sample from\n",
    "        kmeans: an sklearn.cluster.MiniBatchKMeans object that has been fit\n",
    "            to the training data\n",
    "    \n",
    "    returns:\n",
    "        bow: an (n x k) numpy array - n images and k bins (number of clusters\n",
    "            in kmeans)\n",
    "    '''\n",
    "    bow = []\n",
    "    for i in tqdm.tqdm(img_ids, desc=dataset.__class__.__name__):\n",
    "        kp, desc = dataset.getSIFTfeatures(i)\n",
    "        desc = kmeans.predict(desc)\n",
    "        desc = np.bincount(desc, minlength=kmeans.n_clusters)\n",
    "        desc = desc / desc.sum()\n",
    "        bow.append(desc)\n",
    "    bow = stack(bow, 0)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is pretty trivial, so I'm just giving it to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUB_Dataset: 100%|███████████████████████████| 100/100 [00:04<00:00, 21.33it/s]\n",
      "Flowers_Dataset: 100%|███████████████████████| 100/100 [00:09<00:00, 10.41it/s]\n"
     ]
    }
   ],
   "source": [
    "cub_trn_bow = get_bows(cub_data, cub_train[:num_img], kmeans)\n",
    "flower_trn_bow = get_bows(flower_data, flower_train[:num_img], kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('user-data/cub-train-bow.npy', cub_trn_bow)\n",
    "np.save('user-data/flower-train-bow.npy', flower_trn_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cub_trn_bow = np.load('user-data/cub-train-bow.npy')\n",
    "flower_trn_bow = np.load('user-data/flower-train-bow.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Train the Detection Classifier\n",
    "\n",
    "To perform detection, you will be using supervised machine learning to train a binary classifier using the labeled data that you're given (positive image windows with the object to be detected, negative image windows without the object).  We will be using Support Vector Machines (SVMs) as our classifier for this project. To train the SVM, you provide a set of feature vectors in the form of a matrix (the size is $N\\times K$, where $N$ is the total number of training images, both positive and negative) and a corresponding label vector (generally $+1$ at the indices where the feature vector is from the positive set and $-1$ corresponding to the indices of the negative feature vectors).\n",
    "\n",
    "Resources that you may find useful:\n",
    "* [Support Vector Machines (sklearn.svm)](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "    * [User Guide](https://scikit-learn.org/stable/modules/svm.html#svm) and  [Tips on Practical Use](https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use)\n",
    "    * [Another Tutorial](https://www.learnopencv.com/svm-using-scikit-learn-in-python/)\n",
    "* [OpenCV SVM Class](https://docs.opencv.org/3.4.1/d1/d2d/classcv_1_1ml_1_1SVM.html)\n",
    "    * [OpenCV SVM Tutorial](https://docs.opencv.org/3.4/d3/d02/tutorial_py_svm_index.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need to load up descriptors for our test set -- now's a good time to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUB_Dataset: 100%|███████████████████████████| 100/100 [00:06<00:00, 15.39it/s]\n",
      "Flowers_Dataset: 100%|███████████████████████| 100/100 [00:09<00:00, 10.35it/s]\n"
     ]
    }
   ],
   "source": [
    "cub_tst_bow = get_bows(cub_data, cub_test[:num_img], kmeans)\n",
    "flower_tst_bow = get_bows(flower_data, flower_test[:num_img], kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('user-data/cub-test-bow.npy', cub_tst_bow)\n",
    "np.save('user-data/flower-test-bow.npy', flower_tst_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cub_tst_bow = np.load('user-data/cub-test-bow.npy')\n",
    "flower_tst_bow = np.load('user-data/flower-test-bow.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP THE DATA\n",
    "train_data = np.concatenate((cub_trn_bow, flower_trn_bow), 0)\n",
    "train_labels = np.concatenate([\n",
    "    np.ones(cub_trn_bow.shape[0]), np.zeros(flower_trn_bow.shape[0])])\n",
    "\n",
    "test_data = np.concatenate((cub_tst_bow, flower_tst_bow),0)\n",
    "test_labels = np.concatenate([\n",
    "    np.ones(cub_tst_bow.shape[0]), np.zeros(flower_tst_bow.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN THE SVM CLASSIFIER\n",
    "# The LinearSVC is a really fast implementation of an SVM with a linear kernel.\n",
    "# You can get decent results with it. The SVC is slower to train, but you can\n",
    "# choose non-linear kernels which may give better results.\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972\n",
      "0.9321428571428572\n"
     ]
    }
   ],
   "source": [
    "# The score function will tell you the overall classification accuracy of\n",
    "# the classifier on a set of data.\n",
    "print(svm.score(train_data, train_labels))\n",
    "print(svm.score(test_data, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Evaluation\n",
    "\n",
    "Recall the two test sets that were described above. We will use them for different ways to measure the effectivness of our algorithm:\n",
    "\n",
    "* Set A - This set contains thousands of images, each containing either a bird or something else. Set A is to gauge the accuracy of your detector.  _You will not need to do sliding window on this set. Use the full image and simply predict bird vs. not bird_.\n",
    "\n",
    "* ~Set B - This set contains only 12 images, but each image has multiple birds present. On this set, you *will* use sliding window detection, trying to detect the locations of each bird present.~\n",
    "\n",
    "For Set A, you will generate a Precision-Recall curve. Accuracy will be judged by the Area Under the Curve (AUC) metric.  You should use `scikit-learn`'s [`auc()` function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html#sklearn.metrics.auc) to compute this value.  For The PR curves themselves, however, you will need to do the calculations of the curve yourself. You may use scikit-learn's built-in curve for comparison when debugging, but your final results must use your implementation.\n",
    "\n",
    "The student in the class with the best accuracy on Test Set A, as measured by AUC, will receive 10 points of extra credit.\n",
    "\n",
    "Resources for P-R Curves that you may find useful:\n",
    "* scikit-learn's page on [Precision-Recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n",
    "* scikit-learn's [`precision-recall-curve()` function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve)\n",
    "\n",
    "~For Set B, you will simply run your sliding window on each image, noting the total number of birds you detect per image. Plot your bounding boxes for each image in the dataset.~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating a P-R Curve.** You will need a continuous-valued score for each image in the test set -- not just a binary classification decision. You can use the `svm.decision_function` function to calculate these values. To generate the curve, sort the samples by their score -- highest to lowest. For a variety of different thresholds, calculate the precision and recall. The threshold just means that you call anything with a higher score a \"positive\" (label 1). \n",
    "\n",
    "Note that as your threshold gets lower, your precision will tend downward and your recall will tend upward. You'll have minimum recall at your maximum threshold, and minimum precision at your minimum threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1168c4392b0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CALCULATE A P-R CURVE FOR YOUR TEST DATA, AND MEASURE AUC\n",
    "# While at first it may seem like you need to loop over a bunch of threshold\n",
    "# values, there is actually a nice vectorized way to do this (surprise!). Can\n",
    "# you figure out what it is? Hint: use every image's score as a different\n",
    "# threshold.\n",
    "\n",
    "from sklearn.metrics import auc, precision_recall_curve, plot_precision_recall_curve\n",
    "test_scores = svm.decision_function(test_data)\n",
    "\n",
    "indexes = argsort(test_scores)\n",
    "labels, thresholds = test_labels[indexes], test_scores[indexes]\n",
    "xx, yy = numpy.meshgrid(range(len(thresholds)), range(len(thresholds)))\n",
    "putative_labels = (xx >= yy).astype(float) # labels at each threshold\n",
    "\n",
    "tp = (putative_labels * labels[None, :]).sum(axis=1)\n",
    "fp = (putative_labels * (1 - labels[None, :])).sum(axis=1)\n",
    "fn = ((1 - putative_labels) * labels[None, :]).sum(axis=1)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "xlabel(\"Recall\")\n",
    "ylabel(\"Precision\")\n",
    "title(\"auc={:.4f}\".format(auc(recall, precision)))\n",
    "plot(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Challenges\n",
    "Points for this assigment will be assigned as follows (100 points total):\n",
    "* [20 pts] Extract and Cluster Feature Descriptors.\n",
    "* [20 pts] Vector Quantization - Represent each Image as a Bag-of-Words.\n",
    "* [25 pts] Train the Detection Classifier.\n",
    "* [35 pts] Evaluation\n",
    "  - Include a plot of your P-R Curve\n",
    "  - Clearly report your AUC accuracy\n",
    "\n",
    "*As noted above, the person with the highest detection accuracy on Test Set A will receive 10 points extra credit.*\n",
    "\n",
    "### OPTIONAL Challenge\n",
    "\n",
    "1) [20 points extra credit] Use color histograms to improve detection performance.  You can generate a color histogram and concatenate it onto the Bag-of-Words representation for a given image.  Concatenating features is very common.  If you choose to do this, please plot 3 curves in the same figure: the BofW by itself, the Color Histograms by themselves and the combined BoFW/ColorHist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write-up:\n",
    "Provide an explanation for the following items:\n",
    "* Which part of this lab did you find most difficult?\n",
    "* How well did you do on test set A? What additional things would you have done to improve the AUC?\n",
    "* How well did you detect multiple birds in test set B? How would you increase your accuracy of detection in images like these?\n",
    "* What improvements would you recommend for this lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Write-up\n",
    "* The most difficult part of the lab was calculating the P-R curve.  This was the most difficult part because it was basically the only part we had to do ourselves.\n",
    "* I had decent results (over 84% accuracy).  My AUC was about 0.938, which I thought was pretty good.  I noticed that using more images in the training set did the most to improve my accuracy and AUC.\n",
    "* This part was not required.\n",
    "* This lab was fairly easy to do, but took a long time to get descriptors and histograms for the images.  It would be nice to have a quicker way to do that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
